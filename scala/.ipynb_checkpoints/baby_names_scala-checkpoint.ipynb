{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://gda.fios-router.home:4041\n",
       "SparkContext available as 'sc' (version = 3.2.0, master = local[*], app id = local-1640458512011)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "println(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "// https://www.ssa.gov/oact/babynames/limits.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "babyText: org.apache.spark.rdd.RDD[String] = /home/gda/Downloads/namesbystate/*.TXT MapPartitionsRDD[5] at textFile at <console>:24\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val babyText = sc.textFile(\"/home/gda/Downloads/namesbystate/*.TXT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Baby\n",
       "baby: org.apache.spark.rdd.RDD[Baby] = MapPartitionsRDD[7] at map at <console>:27\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Baby(state:String, gender:String, period:Int, name:String, total:Int)\n",
    "val baby = babyText.map(s=>s.split(\",\")).map(s=>Baby(s(0), s(1), s(2).toInt, s(3), s(4).toInt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pivotDf: org.apache.spark.sql.DataFrame = [state: string, gender: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pivotDf = baby.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- period: integer (nullable = false)\n",
      " |-- name: string (nullable = true)\n",
      " |-- total: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivotDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.hadoop.mapred.InvalidInputException",
     "evalue": " Input Pattern file:/home/gda/Downloads/namesbystate/*.TXT matches 0 files",
     "output_type": "error",
     "traceback": [
      "org.apache.hadoop.mapred.InvalidInputException: Input Pattern file:/home/gda/Downloads/namesbystate/*.TXT matches 0 files",
      "  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)",
      "  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)",
      "  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)",
      "  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)",
      "  at scala.Option.getOrElse(Option.scala:189)",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:296)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:446)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:429)",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2728)",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:2935)",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:326)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:806)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:765)",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:774)",
      "  ... 38 elided",
      "Caused by: java.io.IOException: Input Pattern file:/home/gda/Downloads/namesbystate/*.TXT matches 0 files",
      "  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:280)",
      "  ... 87 more",
      ""
     ]
    }
   ],
   "source": [
    "pivotDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby.toDF().groupBy(\"state\").pivot(\"gender\").sum(\"total\").withColumn(\"ALL\", col(\"M\") + col(\"F\")).sort(desc(\"ALL\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
